{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of BioBERT-NER.ipynb","provenance":[{"file_id":"18g_Z6tufHi_TPdVNO45KBcLRSr_k-mUQ","timestamp":1603822989483}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","metadata":{"id":"2fq0mP-oVEtT"},"source":["#!pip install urllib3==1.25.10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LtTceid1VgIU"},"source":["#!pip install pytorch-pretrained-bert\n","#!pip install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t8Gn2SVI7utX"},"source":["import os\n","import re\n","import csv\n","import itertools\n","\n","import nltk\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm, trange\n","from collections import defaultdict, OrderedDict\n","\n","import torch\n","import torch.nn as nn\n","\n","from torch.utils.data import TensorDataset, DataLoader\n","from torch.utils.data import RandomSampler, SequentialSampler\n","\n","from pytorch_pretrained_bert import BertModel, BertTokenizer, BertConfig\n","\n","from transformers import BertForTokenClassification, AdamW\n","from transformers import get_linear_schedule_with_warmup\n","\n","import tensorflow as tf\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tOeVnOOSfJec","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656542146451,"user_tz":300,"elapsed":215,"user":{"displayName":"Dibakar Sigdel","userId":"02121085943335793222"}},"outputId":"07ceda35-7046-4410-9011-39f9dbdefb02"},"source":["# Get GPU device name\n","device_name = tf.test.gpu_device_name()\n","\n","if device_name == '/device:GPU:0':\n","  print('Found GPU at: {}'.format(device_name))\n","else:\n","  raise SystemError('GPU device not found')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found GPU at: /device:GPU:0\n"]}]},{"cell_type":"code","metadata":{"id":"soCBkruYfPUc","executionInfo":{"status":"ok","timestamp":1656542147746,"user_tz":300,"elapsed":156,"user":{"displayName":"Dibakar Sigdel","userId":"02121085943335793222"}},"outputId":"59ee8884-5e0d-4fc8-dfcc-263204b2b7ed","colab":{"base_uri":"https://localhost:8080/"}},"source":["# tell Pytorch to use the GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","print('We will use the GPU:', torch.cuda.get_device_name(0))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla P100-PCIE-16GB\n"]}]},{"cell_type":"markdown","metadata":{"id":"STeCPrW4KFpJ"},"source":["# Import BioBERT"]},{"cell_type":"code","metadata":{"id":"U9pgQv8Iffq9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656542178035,"user_tz":300,"elapsed":5101,"user":{"displayName":"Dibakar Sigdel","userId":"02121085943335793222"}},"outputId":"74965ac2-c911-4a37-da64-fe249a730096"},"source":["!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD\" -O biobert_weights && rm -rf /tmp/cookies.txt"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-06-29 22:36:13--  https://docs.google.com/uc?export=download&confirm=t&id=1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD\n","Resolving docs.google.com (docs.google.com)... 74.125.195.102, 74.125.195.100, 74.125.195.139, ...\n","Connecting to docs.google.com (docs.google.com)|74.125.195.102|:443... connected.\n","HTTP request sent, awaiting response... 303 See Other\n","Location: https://doc-10-20-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/9v2a1sk60akkcb3fgpnlmtk8csk5tsk2/1656542100000/13799006341648886493/*/1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD?e=download&uuid=361ed79d-67b3-48ac-840a-680e55d09daa [following]\n","Warning: wildcards not supported in HTTP.\n","--2022-06-29 22:36:13--  https://doc-10-20-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/9v2a1sk60akkcb3fgpnlmtk8csk5tsk2/1656542100000/13799006341648886493/*/1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD?e=download&uuid=361ed79d-67b3-48ac-840a-680e55d09daa\n","Resolving doc-10-20-docs.googleusercontent.com (doc-10-20-docs.googleusercontent.com)... 74.125.195.132, 2607:f8b0:400e:c09::84\n","Connecting to doc-10-20-docs.googleusercontent.com (doc-10-20-docs.googleusercontent.com)|74.125.195.132|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 401403346 (383M) [application/x-gzip]\n","Saving to: ‘biobert_weights’\n","\n","biobert_weights     100%[===================>] 382.81M   132MB/s    in 2.9s    \n","\n","2022-06-29 22:36:17 (132 MB/s) - ‘biobert_weights’ saved [401403346/401403346]\n","\n"]}]},{"cell_type":"code","metadata":{"id":"zDpKJWMBEa1G","executionInfo":{"status":"ok","timestamp":1656542184700,"user_tz":300,"elapsed":5087,"user":{"displayName":"Dibakar Sigdel","userId":"02121085943335793222"}},"outputId":"1a8b0cd8-5821-4eaf-d2e2-620ced6d0678","colab":{"base_uri":"https://localhost:8080/"}},"source":["!tar -xzf biobert_weights\n","!ls biobert_v1.1_pubmed/"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["bert_config.json\t\t\tmodel.ckpt-1000000.index  vocab.txt\n","model.ckpt-1000000.data-00000-of-00001\tmodel.ckpt-1000000.meta\n"]}]},{"cell_type":"code","source":["#!cat biobert_v1.1_pubmed/vocab.txt"],"metadata":{"id":"yvLiQVMEikRy"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g5dhA43zEfrB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656539572902,"user_tz":300,"elapsed":584,"user":{"displayName":"Dibakar Sigdel","userId":"02121085943335793222"}},"outputId":"6db9bc60-b4d2-41b8-8ba2-ba381a31318f"},"source":["!transformers-cli convert --model_type bert --tf_checkpoint biobert_v1.1_pubmed/model.ckpt-1000000 --config biobert_v1.1_pubmed/bert_config.json --pytorch_dump_output biobert_v1.1_pubmed/pytorch_model.bin"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"/usr/local/bin/transformers-cli\", line 5, in <module>\n","    from transformers.commands.transformers_cli import main\n","  File \"/usr/local/lib/python3.7/dist-packages/transformers/commands/transformers_cli.py\", line 24, in <module>\n","    from .pt_to_tf import PTtoTFCommand\n","  File \"/usr/local/lib/python3.7/dist-packages/transformers/commands/pt_to_tf.py\", line 20, in <module>\n","    from datasets import load_dataset\n","ModuleNotFoundError: No module named 'datasets'\n"]}]},{"cell_type":"code","metadata":{"id":"_vqdJUV1EiEd","executionInfo":{"status":"ok","timestamp":1656539641958,"user_tz":300,"elapsed":644,"user":{"displayName":"Dibakar Sigdel","userId":"02121085943335793222"}},"outputId":"990d0d91-951f-4b91-f575-43d3015b486a","colab":{"base_uri":"https://localhost:8080/"}},"source":["!ls biobert_v1.1_pubmed/\n","!mv biobert_v1.1_pubmed/bert_config.json biobert_v1.1_pubmed/config.json\n","!ls biobert_v1.1_pubmed/"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["config.json\t\t\t\tmodel.ckpt-1000000.index  vocab.txt\n","model.ckpt-1000000.data-00000-of-00001\tmodel.ckpt-1000000.meta\n","mv: cannot stat 'biobert_v1.1_pubmed/bert_config.json': No such file or directory\n","config.json\t\t\t\tmodel.ckpt-1000000.index  vocab.txt\n","model.ckpt-1000000.data-00000-of-00001\tmodel.ckpt-1000000.meta\n"]}]},{"cell_type":"code","metadata":{"id":"V9-3E8ZPLA8Z","executionInfo":{"status":"ok","timestamp":1656539645432,"user_tz":300,"elapsed":334,"user":{"displayName":"Dibakar Sigdel","userId":"02121085943335793222"}},"outputId":"d36cd8a7-ae30-4eb4-b7ec-e11dd4f6e770","colab":{"base_uri":"https://localhost:8080/"}},"source":["!ls "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["biobert_v1.1_pubmed  biobert_weights  bionlp_tags.csv  gdrive  sample_data\n"]}]},{"cell_type":"markdown","metadata":{"id":"DXPpUMx0fZOh"},"source":["# Data"]},{"cell_type":"code","metadata":{"id":"3cSxrv3pKBx0","executionInfo":{"status":"ok","timestamp":1656538769777,"user_tz":300,"elapsed":21281,"user":{"displayName":"Dibakar Sigdel","userId":"02121085943335793222"}},"outputId":"5ddb6cb0-14fb-47aa-8c32-594d6fdffafb","colab":{"base_uri":"https://localhost:8080/"}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"id":"hPfP8ssmdOeW"},"source":["MAX_LEN = 75\n","BATCH_SIZE = 32\n","tokenizer = BertTokenizer(vocab_file='biobert_v1.1_pubmed/vocab.txt', do_lower_case=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U6aKboidi1NQ","executionInfo":{"status":"ok","timestamp":1656538780290,"user_tz":300,"elapsed":932,"user":{"displayName":"Dibakar Sigdel","userId":"02121085943335793222"}},"outputId":"1279964e-dc48-49bd-a099-e69ba95df291","colab":{"base_uri":"https://localhost:8080/"}},"source":["data = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/Data/bio_ner/bionlp_tags.csv')\n","tag_values = data['tags'].values\n","vocab_len = len(tag_values)\n","print('Entity Types:',vocab_len)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Entity Types: 74\n"]}]},{"cell_type":"code","metadata":{"id":"wIVLtuLMmIC8","executionInfo":{"status":"ok","timestamp":1656538783088,"user_tz":300,"elapsed":166,"user":{"displayName":"Dibakar Sigdel","userId":"02121085943335793222"}},"outputId":"b36f10aa-cddf-47eb-89d6-197ac73241c4","colab":{"base_uri":"https://localhost:8080/"}},"source":["df_tags = pd.DataFrame({'tags':tag_values})\n","df_tags.to_csv('bionlp_tags.csv',index=False)\n","df = pd.read_csv('bionlp_tags.csv')\n","print('Tag Preview:\\n', df)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tag Preview:\n","                       tags\n","0     I-Cellular_component\n","1   E-Gene_or_gene_product\n","2   I-Organism_subdivision\n","3     I-Organism_substance\n","4   B-Gene_or_gene_product\n","..                     ...\n","69                 I-Organ\n","70                S-Cancer\n","71            B-Amino_acid\n","72     S-Anatomical_system\n","73                     PAD\n","\n","[74 rows x 1 columns]\n"]}]},{"cell_type":"code","metadata":{"id":"HcwhPzqRtpEk"},"source":["class SentenceFetch(object):\n","  \n","  def __init__(self, data):\n","    self.data = data\n","    self.sentences = []\n","    self.tags = []\n","    self.sent = []\n","    self.tag = []\n","    \n","    # make tsv file readable\n","    with open(self.data) as tsv_f:\n","      reader = csv.reader(tsv_f, delimiter='\\t')\n","      for row in reader:\n","        if len(row) == 0:\n","          if len(self.sent) != len(self.tag):\n","            break\n","          self.sentences.append(self.sent)\n","          self.tags.append(self.tag)\n","          self.sent = []\n","          self.tag = []\n","        else:\n","          self.sent.append(row[0])\n","          self.tag.append(row[1])   \n","\n","  def getSentences(self):\n","    return self.sentences\n","  \n","  def getTags(self):\n","    return self.tags"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**BioNLP Data**:\n","'BioNLP13PC-chem-IOBES', 'BioNLP13PC-chem-IOB', 'BioNLP13PC-ggp-IOB', 'BioNLP13PC-cc-IOB', 'BioNLP13GE-IOB', 'BioNLP13PC-cc-IOBES', 'BioNLP13PC-IOBES', 'BioNLP13PC-IOB', 'BioNLP13PC-ggp-IOBES', 'BioNLP13GE-IOBES', 'BioNLP13CG-species-IOBES', 'BioNLP13CG-IOB', 'BioNLP13CG-ggp-IOB', 'BioNLP13CG-ggp-IOBES', 'BioNLP13CG-chem-IOBES', 'BioNLP13CG-cell-IOB', 'BioNLP13CG-cell-IOBES', 'BioNLP13CG-IOBES', 'BioNLP13CG-chem-IOB', 'BioNLP13CG-species-IOB', 'BioNLP11ID-chem-IOB', 'BioNLP11ID-IOB', 'BioNLP13CG-cc-IOB', 'BioNLP11ID-ggp-IOB', 'BioNLP11ID-IOBES', 'BioNLP13CG-cc-IOBES', 'BioNLP11ID-chem-IOBES', 'BioNLP11ID-species-IOBES', 'BioNLP11ID-species-IOB', 'BioNLP11ID-ggp-IOBES', 'BioNLP11EPI-IOBES', 'BioNLP09-IOB', 'BioNLP09-IOBES', 'BioNLP11EPI-IOB']"],"metadata":{"id":"nsYyF0PJaM_4"}},{"cell_type":"code","metadata":{"id":"GDeDW2CwgcQ-","executionInfo":{"status":"ok","timestamp":1656538824261,"user_tz":300,"elapsed":168,"user":{"displayName":"Dibakar Sigdel","userId":"02121085943335793222"}},"outputId":"32052cd6-24b5-42ed-c622-21900d1d5003","colab":{"base_uri":"https://localhost:8080/"}},"source":["import os\n","print(os.listdir(r'/content/gdrive/My Drive/Colab Notebooks/Data/bionlp'))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['BioNLP13PC-chem-IOBES', 'BioNLP13PC-chem-IOB', 'BioNLP13PC-ggp-IOB', 'BioNLP13PC-cc-IOB', 'BioNLP13GE-IOB', 'BioNLP13PC-cc-IOBES', 'BioNLP13PC-IOBES', 'BioNLP13PC-IOB', 'BioNLP13PC-ggp-IOBES', 'BioNLP13GE-IOBES', 'BioNLP13CG-species-IOBES', 'BioNLP13CG-IOB', 'BioNLP13CG-ggp-IOB', 'BioNLP13CG-ggp-IOBES', 'BioNLP13CG-chem-IOBES', 'BioNLP13CG-cell-IOB', 'BioNLP13CG-cell-IOBES', 'BioNLP13CG-IOBES', 'BioNLP13CG-chem-IOB', 'BioNLP13CG-species-IOB', 'BioNLP11ID-chem-IOB', 'BioNLP11ID-IOB', 'BioNLP13CG-cc-IOB', 'BioNLP11ID-ggp-IOB', 'BioNLP11ID-IOBES', 'BioNLP13CG-cc-IOBES', 'BioNLP11ID-chem-IOBES', 'BioNLP11ID-species-IOBES', 'BioNLP11ID-species-IOB', 'BioNLP11ID-ggp-IOBES', 'BioNLP11EPI-IOBES', 'BioNLP09-IOB', 'BioNLP09-IOBES', 'BioNLP11EPI-IOB']\n"]}]},{"cell_type":"code","metadata":{"id":"24fTBYV4wNwt"},"source":["corpora = '/content/gdrive/My Drive/Colab Notebooks/Data/bionlp'\n","sentences = []\n","tags = []\n","for subdir, dirs, files in os.walk(corpora):\n","    for file in files:\n","        if file == 'train.tsv':\n","          path = os.path.join(subdir, file)\n","          sent = SentenceFetch(path).getSentences()\n","          tag = SentenceFetch(path).getTags()\n","          sentences.extend(sent)\n","          tags.extend(tag)\n","            \n","sentences = sentences[0:20000]\n","tags = tags[0:20000]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ufETsa4Occ2E","executionInfo":{"status":"ok","timestamp":1656538934821,"user_tz":300,"elapsed":160,"user":{"displayName":"Dibakar Sigdel","userId":"02121085943335793222"}},"outputId":"fc735583-d943-4389-e628-4718d258d3a9","colab":{"base_uri":"https://localhost:8080/"}},"source":["len(sentences)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["20000"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"7Oikx9K-B2f7","executionInfo":{"status":"ok","timestamp":1656538936318,"user_tz":300,"elapsed":164,"user":{"displayName":"Dibakar Sigdel","userId":"02121085943335793222"}},"outputId":"566a78be-0e33-4a5e-ec4f-3c170016460b","colab":{"base_uri":"https://localhost:8080/"}},"source":["print('Sentence Preview:\\n',sentences[0])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sentence Preview:\n"," ['The', 'Cdc6', 'protein', 'is', 'ubiquitinated', 'in', 'vivo', 'for', 'proteolysis', 'in', 'Saccharomyces', 'cerevisiae', '.']\n"]}]},{"cell_type":"code","source":["print('Tag Preview:\\n',tags[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sjNrBoQ4n5F4","executionInfo":{"status":"ok","timestamp":1656538938226,"user_tz":300,"elapsed":171,"user":{"displayName":"Dibakar Sigdel","userId":"02121085943335793222"}},"outputId":"610eaada-afcf-4105-9cb2-e9f53ac1e95c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tag Preview:\n"," ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"]}]},{"cell_type":"code","metadata":{"id":"zDczy6Tyw-dw"},"source":["def tok_with_labels(sent, text_labels):\n","  '''tokenize and keep labels intact'''\n","  tok_sent = []\n","  labels = []\n","  for word, label in zip(sent, text_labels):\n","    tok_word = tokenizer.tokenize(word)\n","    n_subwords = len(tok_word)\n","\n","    tok_sent.extend(tok_word)\n","    labels.extend([label] * n_subwords)\n","  return tok_sent, labels\n","\n","tok_texts_and_labels = [tok_with_labels(sent, labs) for sent, labs in zip(sentences, tags)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lH1VCcKpyDaL"},"source":["tok_texts = [tok_label_pair[0] for tok_label_pair in tok_texts_and_labels]\n","labels = [tok_label_pair[1] for tok_label_pair in tok_texts_and_labels]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yHPtXTIByG7x","executionInfo":{"status":"ok","timestamp":1656538979417,"user_tz":300,"elapsed":142,"user":{"displayName":"Dibakar Sigdel","userId":"02121085943335793222"}},"outputId":"57309375-3c73-4cfa-e2bb-f279b9074c73","colab":{"base_uri":"https://localhost:8080/"}},"source":["len(tok_texts)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["20000"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"9pZrjN1uyNVl"},"source":["input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tok_texts],\n","                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n","                          truncating=\"post\", padding=\"post\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SSIuqkC1yQHZ","executionInfo":{"status":"ok","timestamp":1656538981907,"user_tz":300,"elapsed":165,"user":{"displayName":"Dibakar Sigdel","userId":"02121085943335793222"}},"outputId":"c0d4fd72-1d77-4229-fae0-c9e8a7cda240","colab":{"base_uri":"https://localhost:8080/"}},"source":["for char in tok_texts:\n","    print('WordPiece Tokenizer Preview:\\n', char)\n","    break"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WordPiece Tokenizer Preview:\n"," ['The', 'C', '##d', '##c', '##6', 'protein', 'is', 'u', '##bi', '##qui', '##tina', '##ted', 'in', 'v', '##ivo', 'for', 'pro', '##te', '##oly', '##sis', 'in', 'Sa', '##cc', '##har', '##omy', '##ces', 'c', '##ere', '##vis', '##iae', '.']\n"]}]},{"cell_type":"code","metadata":{"id":"tIsQlfGaDwJB"},"source":["tag_values = list(set(itertools.chain.from_iterable(tags)))\n","tag_values.append(\"PAD\")\n","\n","tag2idx = {t: i for i,t in enumerate(tag_values)}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OUlloNl1BCOL"},"source":["tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n","                     maxlen=MAX_LEN, value=tag2idx[\"PAD\"], padding=\"post\",\n","                     dtype=\"long\", truncating=\"post\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WegdV9MEBGPa"},"source":["# attention masks make explicit reference to which tokens are actual words vs padded words\n","attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XHl9n7J7BJOb"},"source":["tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags,\n","                                                            random_state=2018, test_size=0.1)\n","tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n","                                             random_state=2018, test_size=0.1)\n","\n","tr_inputs = torch.tensor(tr_inputs)\n","val_inputs = torch.tensor(val_inputs)\n","tr_tags = torch.tensor(tr_tags)\n","val_tags = torch.tensor(val_tags)\n","tr_masks = torch.tensor(tr_masks)\n","val_masks = torch.tensor(val_masks)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xVJvgMr2BL9j"},"source":["train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n","\n","valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n","valid_sampler = SequentialSampler(valid_data)\n","valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hGlRTtznfV_L"},"source":["# Model"]},{"cell_type":"code","source":["!ls biobert_v1.1_pubmed"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dVUSrkzXo_3g","executionInfo":{"status":"ok","timestamp":1656539663514,"user_tz":300,"elapsed":350,"user":{"displayName":"Dibakar Sigdel","userId":"02121085943335793222"}},"outputId":"cd8efe20-ac19-429a-9c6e-eaf5ba6567b1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["config.json\t\t\t\tmodel.ckpt-1000000.index  vocab.txt\n","model.ckpt-1000000.data-00000-of-00001\tmodel.ckpt-1000000.meta\n"]}]},{"cell_type":"code","metadata":{"id":"IyG2ydyOBadA"},"source":["config = BertConfig.from_json_file('biobert_v1.1_pubmed/config.json')\n","tmp_d = torch.load('biobert_v1.1_pubmed/pytorch_model.bin', map_location=device)\n","state_dict = OrderedDict()\n","\n","for i in list(tmp_d.keys())[:199]:\n","    x = i\n","    if i.find('bert') > -1:\n","        x = '.'.join(i.split('.')[1:])\n","    state_dict[x] = tmp_d[i]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9bLYEb_krO8m"},"source":["#state_dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FSFH01a3bSIg"},"source":["class BioBertNER(nn.Module):\n","\n","  def __init__(self, vocab_len, config, state_dict):\n","    super().__init__()\n","    self.bert = BertModel(config)\n","    self.bert.load_state_dict(state_dict)\n","    self.dropout = nn.Dropout(p=0.3)\n","    self.output = nn.Linear(self.bert.config.hidden_size, vocab_len)\n","    self.softmax = nn.Softmax(dim=1)\n","\n","  def forward(self, input_ids, attention_mask):\n","    encoded_layer, _ = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","    encl = encoded_layer[-1]\n","    out = self.dropout(encl)\n","    out = self.output(out)\n","    return out, out.argmax(-1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cRFqxzw0hH5j"},"source":["model = BioBertNER(vocab_len,config,state_dict)\n","model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7lF1ZFbWqNUm"},"source":["param_optimizer = list(model.named_parameters())\n","no_decay = ['bias', 'gamma', 'beta']\n","optimizer_grouped_parameters = [\n","        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n","         'weight_decay_rate': 0.01},\n","        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n","         'weight_decay_rate': 0.0}\n","    ]\n","\n","optimizer = AdamW(\n","    optimizer_grouped_parameters,\n","    lr=3e-5,\n","    eps=1e-8\n",")\n","epochs = 3\n","max_grad_norm = 1.0\n","\n","total_steps = len(train_dataloader) * epochs\n","\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=total_steps\n",")\n","loss_fn = nn.CrossEntropyLoss().to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xmvcc45KPunQ"},"source":["def train_epoch(model,data_loader,loss_fn,optimizer,device,scheduler):\n","    model = model.train()\n","    losses = []\n","    correct_predictions = 0\n","    for step,batch in enumerate(data_loader):\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","        outputs,y_hat = model(b_input_ids,b_input_mask)\n","        \n","        _,preds = torch.max(outputs,dim=2)\n","        outputs = outputs.view(-1,outputs.shape[-1])\n","        b_labels_shaped = b_labels.view(-1)\n","        loss = loss_fn(outputs,b_labels_shaped)\n","        correct_predictions += torch.sum(preds == b_labels)\n","        losses.append(loss.item())\n","        \n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n","        optimizer.step()\n","        scheduler.step()\n","        optimizer.zero_grad()\n","        \n","    return correct_predictions.double()/len(data_loader) , np.mean(losses)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l3XQ10okPyIa"},"source":["def model_eval(model,data_loader,loss_fn,device):\n","    model = model.eval()\n","    \n","    losses = []\n","    correct_predictions = 0\n","    \n","    with torch.no_grad():\n","        for step, batch in enumerate(data_loader):\n","            batch = tuple(t.to(device) for t in batch)\n","            b_input_ids, b_input_mask, b_labels = batch\n","        \n","            outputs,y_hat = model(b_input_ids,b_input_mask)\n","        \n","            _,preds = torch.max(outputs,dim=2)\n","            outputs = outputs.view(-1,outputs.shape[-1])\n","            b_labels_shaped = b_labels.view(-1)\n","            loss = loss_fn(outputs,b_labels_shaped)\n","            correct_predictions += torch.sum(preds == b_labels)\n","            losses.append(loss.item())\n","        \n","    \n","    return correct_predictions.double()/len(data_loader) , np.mean(losses)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o6_FdiWSQWiB"},"source":["%%time\n","history = defaultdict(list)\n","best_accuracy = 0\n","normalizer = BATCH_SIZE*MAX_LEN\n","loss_values = []\n","\n","for epoch in range(epochs):\n","    \n","    total_loss = 0\n","    print(f'======== Epoch {epoch+1}/{epochs} ========')\n","    train_acc,train_loss = train_epoch(model,train_dataloader,loss_fn,optimizer,device,scheduler)\n","    train_acc = train_acc/normalizer\n","    print(f'Train Loss: {train_loss} Train Accuracy: {train_acc}')\n","    total_loss += train_loss.item()\n","    \n","    avg_train_loss = total_loss / len(train_dataloader)  \n","    loss_values.append(avg_train_loss)\n","    \n","    val_acc,val_loss = model_eval(model,valid_dataloader,loss_fn,device)\n","    val_acc = val_acc/normalizer\n","    print(f'Val Loss: {val_loss} Val Accuracy: {val_acc}')\n","    \n","    history['train_loss'].append(train_loss)\n","    history['train_acc'].append(train_acc)\n","    \n","    history['val_loss'].append(val_loss)\n","    history['val_acc'].append(val_acc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cyow1_IfFEB9"},"source":["sns.set(style='darkgrid')\n","\n","sns.set(font_scale=1.5)\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","# learning curve\n","plt.plot(loss_values, 'b-o')\n","\n","plt.title(\"Training loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rZO9phN3eivG"},"source":["# Test"]},{"cell_type":"code","metadata":{"id":"-ZQuYREzd_EV"},"source":["text = \"\"\"In addition to their essential catalytic role in protein biosynthesis, aminoacyl-tRNA synthetases participate in numerous other functions, including regulation of gene expression and amino acid biosynthesis via transamidation pathways. Herein, we describe a class of aminoacyl-tRNA synthetase-like (HisZ) proteins based on the catalytic core of the contemporary class II histidyl-tRNA synthetase whose members lack aminoacylation activity but are instead essential components of the first enzyme in histidine biosynthesis ATP phosphoribosyltransferase (HisG). Prediction of the function of HisZ in Lactococcus lactis was assisted by comparative genomics, a technique that revealed a link between the presence or the absence of HisZ and a systematic variation in the length of the HisG polypeptide. HisZ is required for histidine prototrophy, and three other lines of evidence support the direct involvement of HisZ in the transferase function. (i) Genetic experiments demonstrate that complementation of an in-frame deletion of HisG from Escherichia coli (which does not possess HisZ) requires both HisG and HisZ from L. lactis. (ii) Coelution of HisG and HisZ during affinity chromatography provides evidence of direct physical interaction. (iii) Both HisG and HisZ are required for catalysis of the ATP phosphoribosyltransferase reaction. This observation of a common protein domain linking amino acid biosynthesis and protein synthesis implies an early connection between the biosynthesis of amino acids and proteins.\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3ixSMp8Zd1qg"},"source":["sent_text = nltk.sent_tokenize(text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IQJDBjDOeXzZ"},"source":["tokenized_text = []\n","for sentence in sent_text:\n","    tokenized_text.append(nltk.word_tokenize(sentence))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yDysDKAJeaUt"},"source":["def tokenize_and_preserve(sentence):\n","    tokenized_sentence = []\n","    \n","    for word in sentence:\n","        tokenized_word = tokenizer.tokenize(word)   \n","        tokenized_sentence.extend(tokenized_word)\n","\n","    return tokenized_sentence"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SsCunKjVgC1d"},"source":["tok_texts = [\n","    tokenize_and_preserve(sent) for sent in tokenized_text\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZDapUO6oefFo"},"source":["input_ids = [tokenizer.convert_tokens_to_ids(txt) for txt in tok_texts]\n","input_attentions = [[1]*len(in_id) for in_id in input_ids]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OS7KQdGqerGo"},"source":["tokens = tokenizer.convert_ids_to_tokens(input_ids[1])\n","new_tokens, new_labels = [], []\n","for token in tokens:\n","    if token.startswith(\"##\"):\n","        new_tokens[-1] = new_tokens[-1] + token[2:]\n","    else:\n","        \n","        new_tokens.append(token)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gw23WTdMeuB8"},"source":["actual_sentences = []\n","pred_labels = []\n","for x,y in zip(input_ids,input_attentions):\n","    x = torch.tensor(x).cuda()\n","    y = torch.tensor(y).cuda()\n","    x = x.view(-1,x.size()[-1])\n","    y = y.view(-1,y.size()[-1])\n","    with torch.no_grad():\n","        _,y_hat = model(x,y)\n","    label_indices = y_hat.to('cpu').numpy()\n","    \n","    tokens = tokenizer.convert_ids_to_tokens(x.to('cpu').numpy()[0])\n","    new_tokens, new_labels = [], []\n","    for token, label_idx in zip(tokens, label_indices[0]):\n","        if token.startswith(\"##\"):\n","            new_tokens[-1] = new_tokens[-1] + token[2:]\n","        else:\n","            new_labels.append(tag_values[label_idx])\n","            new_tokens.append(token)\n","    actual_sentences.append(new_tokens)\n","    pred_labels.append(new_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X12WbCxJYxb8"},"source":["for token, label in zip(actual_sentences, pred_labels):\n","    for t,l in zip(token,label):\n","        print(\"{}\\t{}\".format(t, l))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A1f42dmKPhOW"},"source":["model_save = 'BIONER_classifier.pt'\n","path = F\"models/{model_save}\" \n","torch.save(model.state_dict(), path)"],"execution_count":null,"outputs":[]}]}